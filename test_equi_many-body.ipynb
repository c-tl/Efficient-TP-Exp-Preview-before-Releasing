{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficiency Comparison Experiment (3): Equivariant Many-Body Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tlchen/anaconda3/envs/mace-ef/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from e3nn import o3\n",
    "from e3nn.o3 import TensorProduct, Irreps\n",
    "from e3nn.util.codegen import CodeGenMixin\n",
    "from opt_einsum import contract\n",
    "import collections\n",
    "from typing import List, Union, Dict, Optional\n",
    "\n",
    "from sh2f import sh2f_batch_channel\n",
    "from f2sh import f2sh_batch_channel\n",
    "from fft import FFT_batch_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "const_wigner2gaunt = torch.load(\"constants/const_wigner2gaunt.pt\")\n",
    "sh2f_bases_dict = torch.load(\"constants/coefficient_sh2f.pt\")\n",
    "f2sh_bases_dict = torch.load(\"constants/coefficient_f2sh.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "_TP = collections.namedtuple(\"tp\", \"op, args\")\n",
    "_INPUT = collections.namedtuple(\"input\", \"tensor, start, stop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _wigner_nj(\n",
    "    irrepss: List[o3.Irreps],\n",
    "    normalization: str = \"none\",\n",
    "    filter_ir_mid=None,\n",
    "    dtype=None,\n",
    "    device=None,\n",
    "):\n",
    "    irrepss = [o3.Irreps(irreps) for irreps in irrepss]\n",
    "    if filter_ir_mid is not None:\n",
    "        filter_ir_mid = [o3.Irrep(ir) for ir in filter_ir_mid]\n",
    "\n",
    "    if len(irrepss) == 1:\n",
    "        (irreps,) = irrepss\n",
    "        ret = []\n",
    "        e = torch.eye(irreps.dim, dtype=dtype, device=device)\n",
    "        i = 0\n",
    "        for mul, ir in irreps:\n",
    "            for _ in range(mul):\n",
    "                sl = slice(i, i + ir.dim)\n",
    "                ret += [(ir, _INPUT(0, sl.start, sl.stop), e[sl])]\n",
    "                i += ir.dim\n",
    "        return ret\n",
    "\n",
    "    *irrepss_left, irreps_right = irrepss\n",
    "    ret = []\n",
    "    for ir_left, path_left, C_left in _wigner_nj(\n",
    "        irrepss_left,\n",
    "        normalization=normalization,\n",
    "        filter_ir_mid=filter_ir_mid,\n",
    "        dtype=dtype,\n",
    "        device=device,\n",
    "    ):\n",
    "        i = 0\n",
    "        for mul, ir in irreps_right:\n",
    "            for ir_out in ir_left * ir:\n",
    "                if filter_ir_mid is not None and ir_out not in filter_ir_mid:\n",
    "                    continue\n",
    "\n",
    "                C = o3.wigner_3j(ir_out.l, ir_left.l, ir.l, dtype=dtype, device=device) * const_wigner2gaunt[ir_out.l, ir_left.l, ir.l]\n",
    "                if normalization == \"component\":\n",
    "                    C *= ir_out.dim ** 0.5\n",
    "                if normalization == \"norm\":\n",
    "                    C *= ir_left.dim ** 0.5 * ir.dim ** 0.5\n",
    "\n",
    "                C = torch.einsum(\"jk,ijl->ikl\", C_left.flatten(1), C)\n",
    "                C = C.reshape(\n",
    "                    ir_out.dim, *(irreps.dim for irreps in irrepss_left), ir.dim\n",
    "                )\n",
    "                for u in range(mul):\n",
    "                    E = torch.zeros(\n",
    "                        ir_out.dim,\n",
    "                        *(irreps.dim for irreps in irrepss_left),\n",
    "                        irreps_right.dim,\n",
    "                        dtype=dtype,\n",
    "                        device=device,\n",
    "                    )\n",
    "                    sl = slice(i + u * ir.dim, i + (u + 1) * ir.dim)\n",
    "                    E[..., sl] = C\n",
    "                    ret += [\n",
    "                        (\n",
    "                            ir_out,\n",
    "                            _TP(\n",
    "                                op=(ir_left, ir, ir_out),\n",
    "                                args=(\n",
    "                                    path_left,\n",
    "                                    _INPUT(len(irrepss_left), sl.start, sl.stop),\n",
    "                                ),\n",
    "                            ),\n",
    "                            E,\n",
    "                        )\n",
    "                    ]\n",
    "            i += mul * ir.dim\n",
    "    return sorted(ret, key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def U_matrix_real(\n",
    "    irreps_in: Union[str, o3.Irreps],\n",
    "    irreps_out: Union[str, o3.Irreps],\n",
    "    correlation: int,\n",
    "    normalization: str = \"none\",\n",
    "    filter_ir_mid=None,\n",
    "    dtype=None,\n",
    "    device=None,\n",
    "):\n",
    "    irreps_out = o3.Irreps(irreps_out)\n",
    "    irrepss = [o3.Irreps(irreps_in)] * correlation\n",
    "    wigners = _wigner_nj(irrepss, normalization, filter_ir_mid, dtype, device)\n",
    "    current_ir = wigners[0][0]\n",
    "    out = []\n",
    "    stack = torch.tensor([], device=device)\n",
    "\n",
    "    for ir, _, base_o3 in wigners:\n",
    "        if ir in irreps_out and ir == current_ir:\n",
    "            stack = torch.cat((stack, base_o3.squeeze().unsqueeze(-1)), dim=-1)\n",
    "            last_ir = current_ir\n",
    "        elif ir in irreps_out and ir != current_ir:\n",
    "            if len(stack) != 0:\n",
    "                out += [last_ir, stack]\n",
    "            stack = base_o3.squeeze().unsqueeze(-1)\n",
    "            current_ir, last_ir = ir, ir\n",
    "        else:\n",
    "            current_ir = ir\n",
    "    out += [last_ir, stack]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Contraction(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        irreps_in: o3.Irreps,\n",
    "        irrep_out: o3.Irreps,\n",
    "        correlation: int,\n",
    "        device: Optional[str] = \"cpu\",\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.dtype = torch.get_default_dtype()\n",
    "        self.num_features = irreps_in.count((0, 1))\n",
    "        self.coupling_irreps = o3.Irreps([irrep.ir for irrep in irreps_in])\n",
    "        self.correlation = correlation\n",
    "        dtype = torch.get_default_dtype()\n",
    "        self.U_tensors = {\n",
    "            nu: U_matrix_real(\n",
    "                irreps_in=self.coupling_irreps,\n",
    "                irreps_out=irrep_out,\n",
    "                correlation=nu,\n",
    "                dtype=dtype,\n",
    "                device=device,\n",
    "            )[-1]\n",
    "            for nu in range(1, correlation + 1)\n",
    "        }\n",
    "\n",
    "        # Tensor contraction equations\n",
    "        self.equation_main = \"...ik,kc,bci -> bc...\"\n",
    "        self.equation_weighting = \"...k,kc->c...\"\n",
    "        self.equation_contract = \"bc...i,bci->bc...\"\n",
    "        self.weights = torch.nn.ParameterDict({})\n",
    "        for i in range(1, correlation + 1):\n",
    "            num_params = self.U_tensors[i].size()[-1]\n",
    "            w = torch.nn.Parameter(\n",
    "                torch.ones(num_params, self.num_features, device=device), requires_grad=False\n",
    "            )\n",
    "            self.weights[str(i)] = w\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        out = contract(\n",
    "            self.equation_main,\n",
    "            self.U_tensors[self.correlation],\n",
    "            self.weights[str(self.correlation)].type(self.dtype),\n",
    "            x,\n",
    "        )\n",
    "        for corr in range(self.correlation - 1, 0, -1):\n",
    "            c_tensor = contract(\n",
    "                self.equation_weighting,\n",
    "                self.U_tensors[corr],\n",
    "                self.weights[str(corr)].type(self.dtype),\n",
    "            )\n",
    "            c_tensor = c_tensor + out\n",
    "            out = contract(self.equation_contract, c_tensor, x)\n",
    "        resize_shape = torch.prod(torch.tensor(out.shape[1:]))\n",
    "        return out.view(out.shape[0], resize_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SymmetricContraction(CodeGenMixin, torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        irreps_in: o3.Irreps,\n",
    "        irreps_out: o3.Irreps,\n",
    "        correlation: int or Dict[str, int],\n",
    "        irrep_normalization: str = \"none\",\n",
    "        path_normalization: str = \"none\",\n",
    "        device: str = \"cpu\",\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.type = torch.float64\n",
    "\n",
    "        if irrep_normalization is None:\n",
    "            irrep_normalization = \"component\"\n",
    "\n",
    "        if path_normalization is None:\n",
    "            path_normalization = \"element\"\n",
    "\n",
    "        assert irrep_normalization in [\"component\", \"norm\", \"none\"]\n",
    "        assert path_normalization in [\"element\", \"path\", \"none\"]\n",
    "\n",
    "        self.irreps_in = o3.Irreps(irreps_in)\n",
    "        self.irreps_out = o3.Irreps(irreps_out)\n",
    "\n",
    "        del irreps_in, irreps_out\n",
    "\n",
    "        if type(correlation) is not tuple:\n",
    "            corr = correlation\n",
    "            correlation = {}\n",
    "            for irrep_out in self.irreps_out:\n",
    "                correlation[irrep_out] = corr\n",
    "\n",
    "        self.contractions = torch.nn.ModuleDict()\n",
    "        for irrep_out in self.irreps_out:\n",
    "            self.contractions[str(irrep_out)] = Contraction(\n",
    "                irreps_in=self.irreps_in,\n",
    "                irrep_out=o3.Irreps(str(irrep_out.ir)),\n",
    "                correlation=correlation[irrep_out],\n",
    "                device=device,\n",
    "            )\n",
    "\n",
    "    def forward(self, x: torch.tensor):\n",
    "        outs = []\n",
    "        for irrep in self.irreps_out:\n",
    "            outs.append(self.contractions[str(irrep)](x))\n",
    "        return torch.cat(outs, dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GauntFullyConnectedTensorProductChannel(TensorProduct):\n",
    "\n",
    "    def __init__(\n",
    "        self, irreps_in1, irreps_in2, irreps_out, irrep_normalization: str = None, path_normalization: str = None, **kwargs\n",
    "    ):\n",
    "        irreps_in1 = o3.Irreps(irreps_in1)\n",
    "        irreps_in2 = o3.Irreps(irreps_in2)\n",
    "        irreps_out = o3.Irreps(irreps_out)\n",
    "\n",
    "        instr = [\n",
    "            (i_1, i_2, i_out, \"uuu\", True, const_wigner2gaunt[ir_out.l, ir_1.l, ir_2.l] ** 2)\n",
    "            for i_1, (_, ir_1) in enumerate(irreps_in1)\n",
    "            for i_2, (_, ir_2) in enumerate(irreps_in2)\n",
    "            for i_out, (_, ir_out) in enumerate(irreps_out)\n",
    "            if ir_out in ir_1 * ir_2\n",
    "        ]\n",
    "        super().__init__(\n",
    "            irreps_in1,\n",
    "            irreps_in2,\n",
    "            irreps_out,\n",
    "            instr,\n",
    "            irrep_normalization=irrep_normalization,\n",
    "            path_normalization=path_normalization,\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e3nn_implementation(L, correlation, input, num_channel):\n",
    "\n",
    "    irreps_in1 = Irreps([(num_channel, (l, (-1)**l)) for l in range(L)])\n",
    "    irreps_in2 = irreps_in1\n",
    "    res = input.clone()\n",
    "    output = input.clone()\n",
    "    cutoff = input.shape[-1]\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.synchronize()\n",
    "    start_time = time.time()\n",
    "\n",
    "    for nu in range(2, correlation + 1):\n",
    "        lmax = nu * (L - 1)\n",
    "        irreps_out = Irreps([(num_channel, (l, (-1)**l)) for l in range(lmax + 1)])\n",
    "        tp = GauntFullyConnectedTensorProductChannel(\n",
    "            irreps_in1, irreps_in2, irreps_out,\n",
    "            irrep_normalization='none', path_normalization='none', \n",
    "            internal_weights = False, shared_weights = True\n",
    "        ).to(device)\n",
    "\n",
    "        output = tp(input, output, weight=torch.ones(tp.weight_numel).to(device))\n",
    "        res += output[:, :cutoff]\n",
    "        irreps_in2 = irreps_out\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.synchronize()\n",
    "    end_time = time.time()  \n",
    "    \n",
    "    return res, end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mace_implementation(L, correlation, input, num_channel):\n",
    "\n",
    "    irreps_in = Irreps([(num_channel, (l, (-1)**l)) for l in range(L)])\n",
    "    irreps_out = Irreps([(num_channel, (l, (-1)**l)) for l in range(L)])\n",
    "    symmetric_contractions = SymmetricContraction(\n",
    "        irreps_in,\n",
    "        irreps_out,\n",
    "        correlation,\n",
    "        device=device,\n",
    "        irrep_normalization = \"none\",\n",
    "        path_normalization = \"none\",\n",
    "    )\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.synchronize()\n",
    "    start_time = time.time()\n",
    "\n",
    "    res = symmetric_contractions(input)\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.synchronize()\n",
    "    end_time = time.time()  \n",
    "    \n",
    "    return res, end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def efficient_implementation(L, correlation, input, sh2f_bases, f2sh_bases_nu, offsets_st, offsets_ed):\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.synchronize()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    fs_out = {}\n",
    "    res = input\n",
    "    fs_out[1] = sh2f_batch_channel(input, sh2f_bases)\n",
    "    for nu in range(2, correlation + 1):\n",
    "        if nu % 2 == 0:\n",
    "            fs_out[nu] = FFT_batch_channel(fs_out[nu//2], fs_out[nu//2])\n",
    "        else:\n",
    "            fs_out[nu] = FFT_batch_channel(fs_out[nu//2], fs_out[nu//2 + 1])\n",
    "        res += f2sh_batch_channel(fs_out[nu], f2sh_bases_nu[nu]).real[:, :, :L, offsets_st[nu]:offsets_ed[nu]]\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.synchronize()\n",
    "    end_time = time.time()  \n",
    "\n",
    "    return res, end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_irreps_4to2(irreps_4D):\n",
    "    L = irreps_4D.shape[2]\n",
    "    irreps_2D = irreps_4D[:, :, 0, L - 1 : L].flatten(start_dim=1)\n",
    "    for l in range(1, L):\n",
    "        irreps_2D = torch.cat((irreps_2D, (irreps_4D[:, :, l, -l + L - 1 : l + L].flatten(start_dim=1))), dim=1)\n",
    "    return irreps_2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_irreps_4to3(irreps_4D):\n",
    "    L = irreps_4D.shape[2]\n",
    "    irreps_3D = irreps_4D[:, :, 0, L - 1 : L].flatten(start_dim=2)\n",
    "    for l in range(1, L):\n",
    "        irreps_3D = torch.cat((irreps_3D, (irreps_4D[:, :, l, -l + L - 1 : l + L].flatten(start_dim=2))), dim=2)\n",
    "    return irreps_3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_input_batch_channel(L, batch, num_channel):\n",
    "    sh_coef_in=torch.rand(batch, num_channel, L, 2 * L - 1).to(device)\n",
    "    for l in range(L):\n",
    "        for m in range(-L+1,L):\n",
    "            if m<-l or m>l:\n",
    "                sh_coef_in[:, :, l, m + L - 1] = 0\n",
    "    e3nn_in = flatten_irreps_4to2(sh_coef_in).to(device)\n",
    "    mace_in = flatten_irreps_4to3(sh_coef_in).to(device)\n",
    "    return e3nn_in, mace_in, sh_coef_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_equi_many_body(L, correlation=3, batch=32, num_channel=128, n_sample=10, err_tolerance=1e-4):\n",
    "\n",
    "    '''\n",
    "    Compare the time and results for different implementation methods.\n",
    "    The input irreps have degrees of [0, L) and batch size of `batch` channels of `num_channel`.\n",
    "    The final results is averaged over `n_sample` experiments over random inputs.\n",
    "    The difference of the results from different method is less than `err_tolerance`.\n",
    "    '''\n",
    "    \n",
    "    sh2f_bases = sh2f_bases_dict[L].to(device)\n",
    "    f2sh_bases_nu = {}\n",
    "    offsets_st, offsets_ed = [0,0], [0,0]\n",
    "    for nu in range(2, correlation + 1):\n",
    "        lmax = nu * (L - 1)\n",
    "        f2sh_bases_nu[nu] = f2sh_bases_dict[lmax + 1].to(device)\n",
    "        offsets_st.append(lmax - L + 1)\n",
    "        offsets_ed.append(lmax + L)\n",
    "\n",
    "    # compare different methods\n",
    "    e3nn_times, mace_times, efficient_times = torch.zeros(n_sample), torch.zeros(n_sample), torch.zeros(n_sample)    \n",
    "    for i in range(n_sample):\n",
    "        e3nn_in, mace_in, sh_coef_in = random_input_batch_channel(L, batch, num_channel)\n",
    "        e3nn_res, e3nn_time = e3nn_implementation(L, correlation, e3nn_in, num_channel)\n",
    "        mace_res, mace_time = mace_implementation(L, correlation, mace_in, num_channel)\n",
    "        efficient_res, efficient_time = efficient_implementation(L, correlation, sh_coef_in, sh2f_bases, f2sh_bases_nu, offsets_st, offsets_ed)\n",
    "        \n",
    "        efficient_res_flatten = flatten_irreps_4to2(efficient_res)\n",
    "        \n",
    "        # TC: compare the results\n",
    "        assert (abs(e3nn_res - mace_res) < err_tolerance).all(), f\"Max Error is {abs(e3nn_res - mace_res).max()}!\"\n",
    "        assert (abs(e3nn_res - efficient_res_flatten) < err_tolerance).all(), f\"Max Error is {abs(e3nn_res - efficient_res_flatten).max()}!\"\n",
    "        assert (abs(mace_res - efficient_res_flatten) < err_tolerance).all(), f\"Max Error is {abs(mace_res - efficient_res_flatten).max()}!\"\n",
    "\n",
    "        e3nn_times[i], mace_times[i], efficient_times[i] = e3nn_time, mace_time, efficient_time\n",
    "    print(\"Sanity Check Passed!\")\n",
    "\n",
    "    # @T.C: compare the time\n",
    "    e3nn_mean, e3nn_std = e3nn_times.mean(), e3nn_times.std()\n",
    "    mace_mean, mace_std = mace_times.mean(), mace_times.std()\n",
    "    efficient_mean, efficient_std = efficient_times.mean(), efficient_times.std()\n",
    "\n",
    "    print(f\"e3nn takes {e3nn_mean*1000:.2f} ± {e3nn_std*1000:.2f} ms\")\n",
    "    print(f\"mace takes {mace_mean*1000:.2f} ± {mace_std*1000:.2f} ms\")\n",
    "    print(f\"Efficient takes {efficient_mean*1000:.2f} ± {efficient_std*1000:.2f} ms\")\n",
    "    print(f\"L = {L}, correlation = {correlation}, batch_size = {batch}, channels = {num_channel}\")\n",
    "    print(f\"efficient is {mace_mean / efficient_mean:.2f} x faster than mace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments across Different Degrees (L), holding correlation (ν = 3) fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Check Passed!\n",
      "e3nn takes 249.73 ± 37.27 ms\n",
      "mace takes 3.00 ± 0.07 ms\n",
      "Efficient takes 1.29 ± 0.04 ms\n",
      "L = 2, correlation = 3, batch_size = 32, channels = 128\n",
      "efficient is 2.32 x faster than mace\n"
     ]
    }
   ],
   "source": [
    "compare_equi_many_body(L=2, correlation=3, err_tolerance=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Check Passed!\n",
      "e3nn takes 493.65 ± 45.74 ms\n",
      "mace takes 4.86 ± 0.22 ms\n",
      "Efficient takes 1.71 ± 0.88 ms\n",
      "L = 3, correlation = 3, batch_size = 32, channels = 128\n",
      "efficient is 2.84 x faster than mace\n"
     ]
    }
   ],
   "source": [
    "compare_equi_many_body(L=3, correlation=3, err_tolerance=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Check Passed!\n",
      "e3nn takes 970.76 ± 76.84 ms\n",
      "mace takes 7.06 ± 0.52 ms\n",
      "Efficient takes 2.27 ± 0.40 ms\n",
      "L = 4, correlation = 3, batch_size = 32, channels = 128\n",
      "efficient is 3.11 x faster than mace\n"
     ]
    }
   ],
   "source": [
    "compare_equi_many_body(L=4, correlation=3, err_tolerance=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Check Passed!\n",
      "e3nn takes 1767.27 ± 149.84 ms\n",
      "mace takes 13.59 ± 1.07 ms\n",
      "Efficient takes 3.80 ± 0.75 ms\n",
      "L = 5, correlation = 3, batch_size = 32, channels = 128\n",
      "efficient is 3.57 x faster than mace\n"
     ]
    }
   ],
   "source": [
    "compare_equi_many_body(L=5, correlation=3, err_tolerance=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Check Passed!\n",
      "e3nn takes 3648.32 ± 986.26 ms\n",
      "mace takes 44.02 ± 4.16 ms\n",
      "Efficient takes 8.60 ± 3.22 ms\n",
      "L = 6, correlation = 3, batch_size = 32, channels = 128\n",
      "efficient is 5.12 x faster than mace\n"
     ]
    }
   ],
   "source": [
    "compare_equi_many_body(L=6, correlation=3, err_tolerance=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments across Different correlation (ν), holding degree (L=2) fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Check Passed!\n",
      "e3nn takes 568.19 ± 166.10 ms\n",
      "mace takes 3.81 ± 1.50 ms\n",
      "Efficient takes 1.41 ± 0.42 ms\n",
      "L = 2, correlation = 4, batch_size = 32, channels = 128\n",
      "efficient is 2.71 x faster than mace\n"
     ]
    }
   ],
   "source": [
    "compare_equi_many_body(L=2, correlation=4, err_tolerance=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Check Passed!\n",
      "e3nn takes 734.46 ± 241.98 ms\n",
      "mace takes 4.55 ± 1.10 ms\n",
      "Efficient takes 2.05 ± 0.40 ms\n",
      "L = 2, correlation = 5, batch_size = 32, channels = 128\n",
      "efficient is 2.21 x faster than mace\n"
     ]
    }
   ],
   "source": [
    "compare_equi_many_body(L=2, correlation=5, err_tolerance=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Check Passed!\n",
      "e3nn takes 1087.00 ± 265.75 ms\n",
      "mace takes 8.25 ± 4.38 ms\n",
      "Efficient takes 2.68 ± 0.37 ms\n",
      "L = 2, correlation = 6, batch_size = 32, channels = 128\n",
      "efficient is 3.08 x faster than mace\n"
     ]
    }
   ],
   "source": [
    "compare_equi_many_body(L=2, correlation=6, err_tolerance=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Check Passed!\n",
      "e3nn takes 1140.07 ± 231.60 ms\n",
      "mace takes 11.40 ± 3.32 ms\n",
      "Efficient takes 3.46 ± 0.89 ms\n",
      "L = 2, correlation = 7, batch_size = 32, channels = 128\n",
      "efficient is 3.29 x faster than mace\n"
     ]
    }
   ],
   "source": [
    "compare_equi_many_body(L=2, correlation=7, err_tolerance=1e-4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
